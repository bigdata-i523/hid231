\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Using Big Data For Fact Checking}

\author{Karthik Vegi}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{2619 East 2nd Street, Apt 11}
  \city{Bloomington, IN 47401} 
  \country{USA}}
\email{kvegi@iu.com}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{kvegi}


\begin{abstract}
In this data age, the sheer volume of data makes it impossible to know what is truth and what is not. Politicians are often misconstruing facts to improve their candidacy. Scientists and advertisers are making false claims to gain business advantage.  The more the false claims penetrate into the internet, especially social media, the more chances are that it is believed to be true. We show how Big Data techniques can be used to spot fake news, false claims made by politicians, advertisers, and scientists.
\end{abstract}

\keywords{i523, hid231, big data, veracity, fact check, data accuracy}

\maketitle

\section{Introduction}

Big Data is playing a crucial role in building a smarter planet. Each and every action that we take leaves a digital footprint. Big Data is lending a great helping hand to crunch this data and make smarter decisions. ``Big Data is at the heart of the smart revolution. It is already completely transforming the way we live, find love, cure cancer, conduct science, improve performance, run cities and countries and operate business \cite{Marr2015}.''  \\
Analyzing data in this digital era where data can come from multiple sources involves reading data from different systems in different formats with different contextual meanings. The data extracted from multiple systems can often contradict each other. It could be biased towards a business or a particular entity. Multiple sources also means conflicting and outdated information which makes it highly inaccurate \cite{Berti-Equille2016}. \\
Validation of facts became a major issue with the recent US election of 2016 where the candidates from both the democratic and republic parties used a lot of factual statements in the debates to put their candidacy and party in a better position. These factual statements if not validated, might give a false edge to the party thus having an effect on the entire nation. While some people take these statements with a pinch of salt, a large set of population often believe it to be true and end up voting for party purely based on the claims made by the respective candidates \cite{www-forbes2}. With so many data sources like social media, print media, and the internet, it is not easy to validate and spot fake news. We need to take the help of the technological advances like Big Data and Artificial Intelligence to handle this problem.\\ 
Data inconsistencies with respect to the sources, interpreting the data out of the context, obsolete data and data that is highly modified from the original are all data veracity problems \cite{Berti-Equille2016}. ``Fake news and fact checking is clearly a data veracity problem. Veracity refers to several quality dimensions related to repairing data inconsistencies and fixing other data quality problems such as duplicates, missing or incomplete data \cite{Berti-Equille2016}.''

\section{Fact Checking as a Big Data Problem}
Often veracity is not just about data quality, it is about data understandability. Fake news is understandable and we can make great sense out of it by careful analysis \cite{www-forbes2}. ``Misinformation dynamics, in fact, is where the big data concept of data veracity and the problem of fake news connect. We are not simply talking about the accidental inaccuracies that make up the bulk of enterprise data quality efforts. On the contrary, fake news is intentional misinformation, and furthermore, it is dynamic \cite{www-forbes2}.''  \\
One straight-forward way to understand and account for the reliability of the sources is to formulate a voting algorithm that labels the source system in which the data item resides thus evaluating the accuracy of the data \cite{www-forbes1}. The problem with this approach is that it is too simple and also it doesn't take into account the other factors such as the data lineage of each source. This means that if multiple sources of the data are all derived from a single other source which is inaccurate, we are wrongly labeling the data source. \\ 
The social networking giants like Facebook and Twitter faced this problem and a lot of fingers were pointed at them for acting as a medium for spreading fake news. Facebook took the initiative to tackle the problem head on by implementing an option where the users can flag the story as either true or false \cite{www-forbes2}. The more false votes s story garners, the less likely it appears on the news feed along with a warning message to the users mentioning that a lot of users have reported the story as false \cite{www-forbes2}. The problem with this approach is that we are giving people a chance to alter truth, also making everyone believe that anything that is not flagged is true which might not always be the case \cite{www-forbes2} . \\ 
In order to solve this problem in a more efficient way, we could combine {\em Big Data} and {\em Artificial Intelligence} techniques that eradicate the possible human-generated errors \cite{www-forbes2}. {\em Google} came up with a new method of scoring web pages based on the accuracy of the facts in which the algorithm assigns documents a trust score taking the context into account, which feeds the overall scoring to determine the search rank without solely relying on the links \cite{www-forbes2}. \\
While the social networking giants have a huge role to play in identifying fake news, each individual should take personal responsibility to check the validity of the data using online tools at their disposal rather than believing it blindly. 

\section{Big Data Techniques for fact checking}

\subsection{Recommendation Based Approaches}
Recommendation based approaches take the help of the community to determine the accuracy and quality of the sources. The reputation of the sources increases as more people agree that the source is reliable. These methods clearly have their shortcomings as people can be influenced by third party agencies to improve the trustworthiness of certain sources \cite{Berti-Equille2016}.

\subsection{Content Based Approaches}
``Content based approaches work by computing a trustworthiness score of a source as a function of the belief in its claims, and then the belief score of each claimed data as a function of trustworthiness of the sources asserting it \cite{Berti-Equille2016}.'' The source quality is initialized and iteratively updated based on the content belief. Various probabilistic methods have been used to tackle other aspects beyond trustworthiness and data belief \cite{Berti-Equille2016}. \\
In one such methodology, the truth discovery problem is transformed into a probabilistic inference model. An iterative algorithm is proposed which computes the posterior distribution of all the values of the sources and finds the one with the maximum probability. The model derives all the possible values reported by the sources and the conflicting values in the data streams and then calculates a score \cite{Zhao2014}. \\
\textbf{Figure 1} illustrates the content based approach for truth discovery in data streams. As there can be heterogeneous sources, first a semantic mapping is employed for the values provided by various sources, such that the values for truth discovery are consistent \cite{Zhao2014}. ``For example, the meaning of the weather conditions \textbf{\textit{rainy}} and \textbf{\textit{wet}} are considered to be the same in weather forecast truth discovery. Also \textbf{\textit{partly sunny}} and \textbf{\textit{mostly cloudy}} are grouped and considered to be the same as \textbf{\textit{clear}} \cite{Zhao2014}.'' \\
``At each time \textit{t}, the system collects a set of conflicting
values for entity \textit{i} as \textbf{\textit{V}} = $\left\{{v1,v2,...,vk}\right\}$ from multiple data sources. Next, the system resolves the conflicts and discovers the true value \textit{v} in \textit{V} based on the current data uncertainty and source
quality. Then, the system updates the data uncertainty and source quality based on the inferred value \textbf{v} and conflicting values \textit{V}. \cite{Zhao2014}''

\begin{figure}
\includegraphics[width=0.7\textwidth]{images/fig1.png}
\caption{Truth Discovery In Data Streams \cite{Zhao2014}}
\end{figure}

\subsection{Evidence Based Approaches}
Evidence based approaches augment the content based approaches by relying on evidence, context and priori knowledge about the data sources \cite{Berti-Equille2016}. Data provenance information may be used in truth discovery computation, as well as external information about the context, the sources, the data or user network \cite{Berti-Equille2016}. This involves checking the dynamics of information in the network and recomputing the truth discovery accordingly \cite{Berti-Equille2016}. 
``The problem with evidence based practice is that outside of areas like health care and aviation is that most people in organizations do not care about having research evidence for almost anything they do. That does not mean they are not interesting in research but they are just not that interested in using the research to change how they do things \cite{www-oxford}''

\section{Automating Fact Checking}
In this digital age, fact checking makes more sense when it is done in real time. ``Politicians and media figures make claims about \textit{facts} all the time, but the new army of fact-checkers can often expose claims that are false, exaggerated or half-truths. The number of active fact-checking websites has grown from 44 a year ago to 64 in 2015, according the Duke Reportersâ€™s Lab \cite{Hassan2015}.'' \\
The delay window between the time when a claim is made and the time when the claim is checked for truth has to be as less as possible. Fact checking takes longer time than traditional journalism. This gives enough time for the politicians and other people to make a claim and get away with it \cite{Hassan2015}

\subsection{Computational Challenges} 
\subsubsection{\textbf{Finding claims to check:}} This constitutes converting raw data to natural language and extracting contextual information such as speaker, time, and occasion \cite{Hassan2015}.
\subsubsection{\textbf{Getting data to check claims:}} This involves evaluating the quality and completeness of sources and mapping them back to the data sources. Integrating multiple sources and cleansing data is an integral part of this step \cite{Hassan2015}.

\subsection{Claimbuster}
\textbf{Claimbuster} is an online tool to check for facts in real time. ``For every sentence spoken by the participants of a presidential debate, Claimbuster determines whether the sentence has a factual claim and whether its truthfulness is important to the public. The calculation is based on machine learning models built from thousands of sentences from past debates labeled by humans. The ranking scores help journalists prioritize their efforts in assessing the veracity of claims. Claimbuster can be expanded to other discourses such as interviews and speeches and also adapted for use with social media \cite{Hassan2015}.'' \\
Claimbuster makes use of a supervised learning approach and breaks the sentences into three categories namely \textit{Non Factual Sentences}, \textit{Unimportant Factual Sentences}, and \textit{Check-worthy Factual Sentences} \cite{Hassan2015}. ``Given a sentence, the objective of Claimbuster is to derive a score that reflects the degree by which the sentence belongs to \textit{Check-worthy Factual Sentences}. Many widely-used classification methods support ranking naturally. For instance, consider a Support Vector Machine (SVM). \textit{Check-worthy Factual Sentences} are treated as positive examples and both \textit{Non Factual Sentences} and \textit{Unimportant Factual Sentences} as negative examples. SVM finds a decision boundary between the two types of training examples and calculates the posterior probability using a decision function. The probability scores of all sentences are used to rank them. This clearly will help the journalists and fact checkers to free up time to focus on more important things like reporting and writing \cite{Hassan2015}.''

\section{Conclusion}

Big Data coupled with Artificial Intelligence and Machine Learning can tackle the fact checking problem. Rather than working in silos, the social networking giants and the search engine giant could work together with researchers to come up with a more effective solution. This ensures that there are no loose ends with respect to the accuracy of the data. This is important because there is a disconnect between data sources at times and not everybody has control and access to data that somebody else owns.
    
\begin{acks}

The author would like to thank Dr. Gregor von Laszewski and the teaching assistants for their support and suggestions in writing this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
