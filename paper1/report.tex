\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
\title{Using Big Data For Fact Checking}

\author{Karthik Vegi}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{2619 East 2nd Street, Apt 11}
  \city{Bloomington, IN 47401} 
  \country{USA}}
\email{kvegi@iu.com}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{kvegi}


\begin{abstract}
Big Data is no more the elephant in the room it once used to be. Since John Mashey coined the term in 1998, it has come a long way. It is often described as the 3 V's: Volume, Velocity, and Variety of the data \cite{www-sas}. Of late, a new dimension, \textit{Veracity} has been gaining importance which describes the quality and accuracy of the data. This paper intends to discuss how Big Data can be used to spot fake news, bad data used by politicians, advertisers, and scientists.
\end{abstract}

\keywords{i523, hid231, big data, veracity, fact check, data accuracy}

\maketitle

\section{Introduction}

Big Data is playing a crucial role in building a smarter planet. Each and every action that we take leaves a digital footprint. Big Data is lending a great helping hand to crunch this data to make smarter decisions. "Big Data is at the heart of the smart revolution. It is already completely transforming the way we live, find love, cure cancer, conduct science, improve performance, run cities and countries and operate business" \cite{Marr2015}. \\
"Large scale searches and analyses over multiple sources involves extracting data from highly heterogeneous structures, semantics, and qualities. One of the fundamental issues is that the extracted information can be biased, noisy, outdated, incorrect, misleading, and thus unreliable. To add to the problem, available data sources can provide conflicting information, leaving the users in doubt with respect to the accuracy" \cite{Berti-Equille2016}. "The impact of fake news on the recent election has focused public attention on this multi-tentacled and growing problem. Vast swaths of the population fall prey to such misinformation, while others struggle to discern unbiased truth from the morass of lies and distortions that surrounds us" \cite{www-forbes2}. 
With so many data sources like media, internet, newspaper, and many more, it is not easy to spot fake news and fack check the data. We need to take the help of the technological advances like Big Data and Artificial Intelligence to handle this problem. "Fake news and fact checking is clearly a data veracity problem. Veracity refers to several quality dimensions related to repairing data inconsistencies and fixing other data quality problems such as duplicates, missing or incomplete data. Data veracity can be attributed to the following: \\
\textbf{Ambiguity}: Data can be inconsistent from one source to another, leading to misinterpretation. \\
\textbf{Staleness}: The data is obsolete and no longer relevant. \\
\textbf{Falsification}: False or distorted information can be intentionally propagated by one source or a coalition of sources. Information can be manipulated or presented selectively to influence the audience and encourage and particular conclusion"\cite{Berti-Equille2016}.

\section{Fact Checking as a Big Data Problem}
Patricia Saporito, Data and Analytics Thought Leader for SAP has this take on data veracity: "veracity is not just about data quality, it is about data \textit{understandability}". Fake news is understandable and we can make great sense out of it by careful analysis. We should therefore strive to achieve \textit{truthfulness} \cite{www-forbes2}. "Misinformation dynamics, in fact, is where the big data concept of data veracity and the problem of fake news connect. We are not simply talking about the accidental inaccuracies that make up the bulk of enterprise data quality efforts. On the contrary, fake news is intentional misinformation, and furthermore, it is dynamic \cite{www-forbes2}." \\
"A common strategy to evaluate the reliability of the sources is to take advantage of data redundancy, and rely on majority voting heuristic, which simply assigns a true label to data that are claimed by the majority of the sources. But this strategy is known to be error-prone, because it counts all the sources equally and does not consider source dependence or collusion \cite{www-forbes1}." \\
The social networking giants Facebook and Twitter faced this problem and a lot of fingers were pointed at them for acting as a medium for spreading fake news. Facebook took the initiative to tackle the problem head on by implementing an option where the users can flag the story as false. The more false votes it garners, the less likely it is for it to appear on the news feed. It also displays a warning to the users mentioning that a lot of users have reported the story as false. But the problem here is that we are giving people a chance to alter truth. It also makes everyone believe that anything that is not flagged is true which might not always be the case. \cite{www-forbes2}. \\
"To solve these problems, a combination of big data and AI methodologies are being developed that rely less on human-generated input, which can be swayed by opinion or a lack of facts. Google published a paper in 2015 about a new method of scoring web pages based on the accuracy of the facts presented. The algorithm assigns documents a trust score, which would then presumably be used as part of Google’s overall scoring to determine search rank. The technology is important, because it is attempting to understand a page’s context without the use of third-party signals, like links. 
The news media and social media cannot be solely responsible for preventing fake news. Each one of us have equal responsibility to discern the accuracy. Tools already exist that can help individual users spot fake news sites. \textit{Hoaxy} is an online tool that helps people visualize the spread of claims and fact-checking online, and is available to anyone to use. Many Chrome extensions have been created that can alert and help filter fake news. Even popular websites like \textit{Snopes} and \textit{FactCheck.org} can help identify the most egregious fake stories \cite{www-forbes2}." 

\section{Conclusion}

Rather than working in silos, the social networking giants and the search engine giant can work together to come up with a more effective solution. This ensures that there are no loose ends with respect to the accuracy of the data. This is important because there is a disconnect between data sources at times. For example, Facebook does not allow Google to enter certain areas of it to make sure it has complete control over the data \cite{www-android}.

\begin{acks}

The author would like to thank Dr. Gregor von Laszewski and the teaching assistants for their support and suggestions in writing this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
